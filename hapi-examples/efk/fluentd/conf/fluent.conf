# bind fluentd on IP 0.0.0.0
# port 24224

<system>
  workers 10 
</system>

<worker 0>
  <source>
    @type tail
    path /fluentd/log/logs/*/eqsurv*/*,/fluentd/log/logs/*/catalina*/*
    path_key file_path
    read_from_head true

    <parse>
      @type multiline
      format_firstline /^\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2},\d{3}|\d{2}-[A-Za-z]{3}-\d{4}\s\d{2}:\d{2}:\d{2}\.\d{3}/
      format1 /((?<timestamp_1>\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2},\d{3})\s\[(?<thread>[^\]]+)\]\s*(?<log_level>[A-Z]+)\s*(?<class>\S+):(?<line>\d+)\s*-\s*(?<message>.*)$)?/
      format2 /((?<timestamp_2>\d{2}-[A-Za-z]{3}-\d{4}\s\d{2}:\d{2}:\d{2}.\d{3})\s*(?<log_level>[^\s]+)\s*\[(?<thread>[^\]]+)\]\s*(?<class>[^\s]+)\.(?<method>[^\s]+)\s\[.*\]\s(?<status>ENTER|EXIT|SQL\s*LOG)\s*:\s*(?<message>.+)$)?/
      format3 /((?<timestamp_1>\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2},\d{3})\s*\[(?<thread>[^\]]+)\]\s*(?<log_level>[^\s]+)\s*(?<class>\S+):(?<line>\d+)\s*-\s*\[(?<sql_type>(?:[^\]])+)\]\s*(?<message>.*)$)?/
    </parse>

    @label @EQSURV_CATALINA
    tag eqsurv_catalina.logs
  </source>

  <label @EQSURV_CATALINA>
    <filter eqsurv_catalina.logs>
      @type record_transformer
      enable_ruby true
      <record>
        @timestamp ${record.has_key?("timestamp_1") ? Time.strptime(record["timestamp_1"], "%Y-%m-%d %H:%M:%S,%L").strftime("%Y-%m-%dT%H:%M:%S%z") : Time.strptime(record["timestamp_2"], "%d-%b-%Y %H:%M:%S.%L").strftime("%Y-%m-%dT%H:%M:%S%z")}
        customer_name ${record['file_path'].match('\/([^\/]+)\/[^\/]+\/[^\/]+\.[^.\/]+$')[1]}
        log_type      ${record['file_path'].match('\/([^\/]+)\/[^\/]+\.[^.\/]+$')[1].split(/[._ !@#$%^&-]/).first.downcase()}
      </record>
      remove_keys timestamp_1, timestamp_2
    </filter>

    <match eqsurv_catalina.logs>
      @type copy
      <store>
        @type elasticsearch
        host elasticsearch
        port 9200
        logstash_format true
        logstash_prefix eqsurv-manager-catalina-log
        logstash_dateformat %Y%m%d
        include_tag_key true
        tag_key @log_name
        type_name eqsurv_log
        flush_interval 2s
      </store>
      <store>
        @type stdout
      </store>
    </match>
  </label>
</worker>

# <worker 0>
#   <source>
#     @type tail
#     path /fluentd/log/logs/*/eqsurv*/*
#     path_key file_path
#     read_from_head true

#     <parse>
#       @type multiline
#       format_firstline /^\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2},\d{3}/
#       format1 /^(?<timestamp_1>\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2},\d{3})\s*\[(?<thread>[^\]]+)\]\s*(?<log_level>[^\s]+)\s*(?<class>\S+):(?<line>\d+)\s*-\s*\[(?<sql_type>[^\]]+)\]\s*(?<message>.*)$/
#     </parse>

#     @label @EQSURV
#     tag eqsurv.logs
#   </source>

#   <label @EQSURV>
#     <filter eqsurv.logs>
#       @type record_transformer
#       enable_ruby true
#       <record>
#         @timestamp ${if record.has_key?('timestampl_1'); Time.strptime(record["timestamp_1"], "%b %d %H:%M:%S").strftime("%Y-%m-%dT%H:%M:%S%z"); end;}
#         customer_name ${record['file_path'].match('\/([^\/]+)\/[^\/]+\/[^\/]+\.log$')[1]}
#         log_type      ${record['file_path'].match('\/([^\/]+)\/[^\/]+\.log$')[1].split(/[._ !@#$%^&-]/).first.downcase()}
#       </record>
#     </filter>

#     <match eqsurv.logs>
#       @type copy
#       <store>
#         @type elasticsearch
#         host elasticsearch
#         port 9200
#         logstash_format true
#         logstash_prefix eqsurv-manager-log
#         logstash_dateformat %Y%m%d
#         include_tag_key true
#         tag_key @log_name
#         type_name eqsurv_log
#         flush_interval 2s
#       </store>
#       <store>
#         @type stdout
#       </store>
#     </match>
#   </label>
# </worker>

# <worker 0>
#   <source>
#     @type tail
#     path /fluentd/log/auth/auth.log
#     read_from_head true
#     <parse>
#       @type syslog
#     </parse>
#     @label @AUTH_LABEL
#     tag os.auth.logs
#   </source>

#   <label @AUTH_LABEL>
#     <filter os.auth.logs>
#       @type record_transformer
#       enable_ruby true
#       <record>
#         @timestamp ${if record.has_key?('@timestamp'); Time.strptime(record["@timestamp"], "%b %d %H:%M:%S").strftime("%Y-%m-%dT%H:%M:%S%z"); end;}
#       </record>
#     </filter>

#     <match os.auth.logs>
#       @type copy
#       <store>
#         @type elasticsearch
#         host elasticsearch
#         port 9200
#         logstash_format true
#         logstash_prefix os-auth-logs
#         logstash_dateformat %Y%m%d
#         include_tag_key true
#         tag_key @log_name
#         type_name access_log
#         flush_interval 2s
#       </store>
#       <store>
#         @type stdout
#       </store>
#     </match>
#   </label>
# </worker>

# <worker 1>
#   <source>
#     @type tail
#     path /fluentd/log/auth/kern.log
#     read_from_head true
#     <parse>
#      @type regexp
#      expression /^(?<@timestamp>\w{3} \d{2} \d{2}:\d{2}:\d{2}) (?<hostname>\S+) kernel: \[(?<timestamp_sec>[\d.]+)\] (?<message>.*)$/
#     </parse>
#     @label @KERN_LABEL
#     tag os.kern.logs
#   </source>

#   <label @KERN_LABEL>
#     <filter os.kern.logs>
#       @type record_transformer
#       enable_ruby true
#       <record>
#         @timestamp ${if record.has_key?('@timestamp'); Time.strptime(record["@timestamp"], "%b %d %H:%M:%S").strftime("%Y-%m-%dT%H:%M:%S%z"); end;}
#       </record>
#     </filter>

#     <match os.kern.logs>
#       @type copy
#       <store>
#         @type elasticsearch
#         host elasticsearch
#         port 9200
#         logstash_format true
#         logstash_prefix os-kern-logs
#         logstash_dateformat %Y%m%d
#         include_tag_key true
#         tag_key @log_name
#         type_name access_log
#         flush_interval 2s
#       </store>
#       <store>
#         @type stdout
#       </store>
#     </match>
#   </label>
# </worker>

# <worker 2>
#   <source>
#     @type tail
#     path /fluentd/log/auth/syslog
#     read_from_head true
#     <parse>
#       @type syslog
#     </parse>
#     @label @SYSTEM_LABEL
#     tag os.system.logs 
#   </source>

#   <label @SYSTEM_LABEL>
#     <filter os.system.logs>
#       @type grep

#       <regexp>
#         key ident
#         pattern /systemd|kernel/
#       </regexp>
#     </filter>

#     <match os.system.logs>
#       @type copy
#       <store>
#         @type elasticsearch
#         host elasticsearch
#         port 9200
#         logstash_format true
#         logstash_prefix os-system-log
#         logstash_dateformat %Y%m%d
#         include_tag_key true
#         type_name access_log
#         tag_key @log_name
#         flush_interval 2s
#       </store>
#       <store>
#         @type stdout
#       </store>
#     </match>
#   </label>
# </worker>

# <worker 3>
#   <source>
#     @type forward
#     port 24224
#     bind 0.0.0.0
#     @label @NORMAL
#   </source>

#   <label @NORMAL>
#     <filter device.data.log.*>
#       @type record_transformer

#       <record>
#         device_id ${record["device_name"]}-${record["device_id"]}
#       </record>
#       remove_keys device_name
#     </filter>

#     <match device.data.log.*>
#       @type copy
#       <store>
#         @type elasticsearch
#         host elasticsearch
#         port 9200
#         logstash_format true
#         logstash_prefix device-data-log
#         logstash_dateformat %Y%m%d
#         include_tag_key true
#         type_name access_log
#         tag_key @log_name
#         flush_interval 2s
#       </store>
#       <store>
#         @type stdout
#       </store>
#     </match>

#     <match access.log.**>
#       @type copy
#       <store>
#         @type elasticsearch
#         host elasticsearch
#         port 9200
#         logstash_format true
#         logstash_prefix access-log
#         logstash_dateformat %Y%m%d
#         include_tag_key true
#         type_name access_log
#         tag_key @log_name
#         flush_interval 2s
#       </store>
#       <store>
#         @type stdout
#       </store>
#     </match>
#   </label>
# </worker>

# <worker 4>
#   <source>
#     @type tail
#     path /fluentd/log/auth/dmesg
#     read_from_head true
#     <parse>
#      @type regexp
#       expression /^\[\s*(?<timestamp>[^\]]*)\]\s*(?<event>[^:]*):\s(?<message>.*)$/
#     </parse>
#     @label @DMESG
#     tag os.dmesg.logs
#   </source>

#   <label @DMESG>
#     <match os.dmesg.logs>
#       @type copy

#       <store>
#         @type elasticsearch
#         host elasticsearch
#         port 9200
#         logstash_format true
#         logstash_prefix os-dmesg-log
#         logstash_dateformat %Y%m%d
#         include_tag_key true
#         type_name access_log
#         tag_key @log_name
#         flush_interval 2s
#       </store>
#       <store>
#         @type stdout
#       </store>
#     </match>
#   </label>
# </worker>

# <worker 5>
#   <source>
#     @type tail
#     path /fluentd/log/auth/apt/history.log*
#     read_from_head true
#       <parse>
#         @type multiline
#         format_firstline /^Start-Date/
#         format1 /Start-Date:\s+(?<started-time>[^\s]+\s+[^\s]+)\n/
#         format2 /(?<command>.*)\n/
#         format3 /End-Date: (?<ended-time>[^\s]+\s+[^\s]+)\n/
#       </parse>
#     @label @HISTORY
#     tag os.history.logs
#   </source>

#   <label @HISTORY>
#     <match os.history.logs>
#       @type copy
#       <store>
#         @type elasticsearch
#         host elasticsearch
#         port 9200
#         logstash_format true
#         logstash_prefix os-history-log
#         logstash_dateformat %Y%m%d
#         include_tag_key true
#         type_name access_log
#         tag_key @log_name
#         flush_interval 2s
#       </store>
#       <store>
#         @type stdout
#       </store>
#     </match>
#   </label>
# </worker>

# <worker 6>
#   <source>
#     @type tail
#     path /fluentd/log/auth/boot.log
#     read_from_head true
#     <parse>
#      @type regexp
#       expression /^\s*\[?[^\s]*\s*(?<response>[^\s]+)\s*[&\s]*\s*(?<status>[^\s]*)\s*(?<message>.*)$/
#     </parse>
#     @label @BOOT
#     tag os.boot.logs
#   </source>
#   <label @BOOT>
#     <match os.boot.logs>
#       @type copy
#       <store>
#         @type elasticsearch
#         host elasticsearch
#         port 9200
#         logstash_format true
#         logstash_prefix os-boot-log
#         logstash_dateformat %Y%m%d
#         include_tag_key true
#         type_name access_log
#         tag_key @log_name
#         flush_interval 2s
#       </store>
#       <store>
#         @type stdout
#       </store>
#     </match>
#   </label>
# </worker>

# <worker 7>
#   <source>
#     @type tail
#     path /fluentd/log/logs/catalia.out
#     read_from_head true

#     <parse>
#       @type multiline
#       format_firstline /^\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2},\d{3}|\d{2}-[A-Za-z]{3}-\d{4}\s\d{2}:\d{2}:\d{2}\.\d{3}/
#       format1 /^(?<timestamp_1>\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2},\d{3})\s\[(?<thread>[^\]]+)\]\s*(?<log_level>[A-Z]+)\s*(?<class>\S+):(?<line>\d+)\s*-\s*(?<message>.*)$|^(?<timestamp_2>\d{2}-[A-Za-z]{3}-\d{4}\s\d{2}:\d{2}:\d{2}.\d{3})\s*(?<log_level>[^\s]+)\s*\[(?<thread>[^\]]+)\]\s*(?<class>[^\s]+)\.(?<method>[^\s]+)\s\[.*\]\s(?<status>ENTER|EXIT|SQL\s*LOG)\s*:\s*(?<message>.+)$/
#     </parse>

#     @label @CATALINA
#     tag catalina.logs
#   </source>
#   <label @CATALINA>
#     <filter catalina.logs>
#       @type record_transformer
#       enable_ruby true

#       <record>
#         @timestamp ${record.has_key?("timestamp_1") ? Time.strptime(record["timestamp_1"], "%Y-%m-%d %H:%M:%S,%L").strftime("%Y-%m-%dT%H:%M:%S%z") : Time.strptime(record["timestamp_2"], "%d-%b-%Y %H:%M:%S.%L").strftime("%Y-%m-%dT%H:%M:%S%z")}
#       </record>
#       
#       remove_keys timestamp_1,timestamp_2
#     </filter>

#     <match catalina.logs>
#       @type copy
#       <store>
#         @type elasticsearch
#         host elasticsearch
#         port 9200
#         logstash_format true
#         logstash_prefix catalina-logs
#         logstash_dateformat %Y%m%d
#         include_tag_key true
#         type_name access_log
#         tag_key @log_name
#         flush_interval 2s
#       </store>

#       <store>
#         @type stdout
#       </store>
#     </match>
#   </label>
# </worker>

# <worker 8>
#   <source>
#     @type tail
#     path /fluentd/log/logs/apache.log
#     read_from_head true

#     <parse>
#      @type regexp
#       expression /^(?<client.ip>\S+) (?<remote_host>\S+) (?<remote_user>\S+) \[(?<@timestamp>[^\]]+)\] "(?<request.method>\S+)(?: +(?<request.url>(?:[^\"]|\\.)*?)(?: +\S*)?)?" (?<response.status_code>\d+) (?<response.size>\d+)$/
#     </parse>

#     @label @APACHE_LOGS
#     tag apache.log
#   </source>
#   <label @APACHE_LOGS>

#     <filter apache.log>
#       @type record_transformer
#       enable_ruby true
#       <record>
#         @timestamp ${if record.has_key?('@timestamp'); Time.strptime(record["@timestamp"], "%d/%b/%Y:%H:%M:%S %z").strftime("%Y-%m-%dT%H:%M:%S%z"); end;}
#       </record>
#     </filter>
#     <match apache.log>
#       @type copy
#       <store>
#         @type elasticsearch
#         host elasticsearch
#         port 9200
#         logstash_format true
#         logstash_prefix apache-log
#         logstash_dateformat %Y%m%d
#         include_tag_key true
#         type_name access_log
#         tag_key @log_name
#         flush_interval 2s
#       </store>
#       <store>
#         @type stdout
#       </store>
#     </match>
#   </label>
# </worker>

# <worker 9>
#   <source>
#     @type tail
#     path /fluentd/log/logs/nginx.log
#     read_from_head true

#     <parse>
#      @type regexp
#       expression /^(?<client.ip>\S+) (?<remote_host>\S+) (?<remote_user>[^ ]*) \[(?<@timestamp>[^\]]*)\] "(?<request.method>\S+)(?: +(?<request.url>(?:[^\"]|\\.)*?)(?: +\S*)?)?" (?<response.status_code>[^ ]*) (?<response.size>[^ ]*)(?: "(?<referer>(?:[^\"]|\\.)*)" "(?<user.agent>(?:[^\"]|\\.)*)")?$/
#     </parse>

#     @label @NGIN_LOGS
#     tag nginx.log
#   </source>
#   <label @NGIN_LOGS>
#     <filter nginx.log>
#       @type record_transformer
#       enable_ruby true
#       <record>
#         @timestamp ${if record.has_key?('@timestamp'); Time.strptime(record["@timestamp"], "%d/%b/%Y:%H:%M:%S %z").strftime("%Y-%m-%dT%H:%M:%S%z"); end;}
#       </record>
#     </filter>
#     <match nginx.log>
#       @type copy
#       <store>
#         @type elasticsearch
#         host elasticsearch
#         port 9200
#         logstash_format true
#         logstash_prefix nginx-log
#         logstash_dateformat %Y%m%d
#         include_tag_key true
#         type_name access_log
#         tag_key @log_name
#         flush_interval 2s
#       </store>
#       <store>
#         @type stdout
#       </store>
#     </match>
#   </label>
# </worker>

# <worker 0>
#   <source>
#     @type tail
#     path /fluentd/log/logs/*/*/*
#     path_key file_path
#     read_from_head true

#     <parse>
#      @type regexp
#       expression /^(?<client.ip>[^ ]*) (?<remote_host>[^ ]*) (?<remote_user>[^ ]*) \[(?<@timestamp>[^\]]*)\] "(?<request.method>\S+)(?: +(?<request.url>(?:[^\"]|\\.)*?)(?: +\S*)?)?" (?<response.status_code>[^ ]*) (?<response.size>[^ ]*)(?: "(?<referer>(?:[^\"]|\\.)*)" "(?<user.agent>(?:[^\"]|\\.)*)")?$/
#       time_format %d/%b/%Y:%H:%M:%S %z
#     </parse>

#     @label @ACCESS_LOGS 
#     tag access.log
#   </source>
#   <label @ACCESS_LOGS>
#     <filter access.log>
#       @type record_transformer
#       enable_ruby true
#       <record>
#         @timestamp ${if record.has_key?('@timestamp'); Time.strptime(record["@timestamp"], "%d/%b/%Y:%H:%M:%S %z").strftime("%Y-%m-%dT%H:%M:%S%z"); end;}
#         response.status_code ${record['response.status_code'].to_i}
#         response.size ${record['response.size'].to_i}
#         customer_name ${record['file_path'].match('\/([^\/]+)\/[^\/]+\/[^\/]+\.log$')[1]}
#         log_type      ${record['file_path'].match('\/([^\/]+)\/[^\/]+\.log$')[1].split(/[._ !@#$%^&-]/).first.downcase()}
#       </record>
#     </filter>

#     <filter access.log>
#       @type geoip
#       geoip_lookup_keys client.ip

#       backend_library geoip2_c

#       <record>
#         geoip.continent_name   ${continent.names.en["client.ip"]}
#         geoip.country_name     ${country.names.en["client.ip"]}
#         geoip.country_iso_code ${country.iso_code["client.ip"]}
#         geoip.city_name        ${city.names.en["client.ip"]}
#         geoip.region_iso_code  ${registered_country.iso_code["client.ip"]}
#         geoip.region_name      ${registered_country.names.en["client.ip"]}
#         geoip.location     'POINT (${location.latitude["client.ip"]} ${location.longitude["client.ip"]})'
#       </record>
#       skip_adding_null_record true
#     </filter>

#     <match access.log>
#       @type copy
#       <store>
#         @type elasticsearch
#         host elasticsearch
#         port 9200
#         logstash_format true
#         logstash_prefix access-log
#         logstash_dateformat %Y%m%d
#         include_tag_key true
#         type_name access_log
#         tag_key @log_name
#         templates { "access-template": "/fluentd/templates/template.json"}
#         template_overwrite true
#         flush_interval 2s
#       </store>
#       <store>
#         @type stdout
#       </store>
#     </match>
#   </label>
# </worker>
