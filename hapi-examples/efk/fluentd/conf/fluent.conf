# bind fluentd on IP 0.0.0.0
# port 24224

<system>
  workers 10 
</system>

# <worker 0>
#   <source>
#     @type tail
#     path /fluentd/log/auth/auth.log
#     read_from_head true
#     <parse>
#      @type regexp
#      expression /^(?<@timestamp>\w{3} \d{2} \d{2}:\d{2}:\d{2}) (?<hostname>\S+) (?<process>[^\[\:]+)(?:\[(?<pid>\d+)\])?: (?<message>.*)$/
#     </parse>
#     @label @AUTH_LABEL
#     tag os.auth.logs
#   </source>

#   <label @AUTH_LABEL>
#     <filter os.auth.logs>
#       @type record_transformer
#       enable_ruby true
#       <record>
#         @timestamp ${if record.has_key?('@timestamp'); Time.strptime(record["@timestamp"], "%b %d %H:%M:%S").strftime("%Y-%m-%dT%H:%M:%S%z"); end;}
#       </record>
#     </filter>

#     <match os.auth.logs>
#       @type copy
#       <store>
#         @type elasticsearch
#         host elasticsearch
#         port 9200
#         logstash_format true
#         logstash_prefix os-auth-logs
#         logstash_dateformat %Y%m%d
#         include_tag_key true
#         tag_key @log_name
#         type_name access_log
#         flush_interval 2s
#       </store>
#       <store>
#         @type stdout
#       </store>
#     </match>
#   </label>
# </worker>

<worker 1>
  <source>
    @type tail
    path /fluentd/log/auth/kern.log
    read_from_head true
    <parse>
     @type regexp
     expression /^(?<@timestamp>\w{3} \d{2} \d{2}:\d{2}:\d{2}) (?<hostname>\S+) kernel: \[(?<timestamp_sec>[\d.]+)\] (?<message>.*)$/
    </parse>
    @label @KERN_LABEL
    tag os.kern.logs
  </source>

  <label @KERN_LABEL>
    <filter os.kern.logs>
      @type record_transformer
      enable_ruby true
      <record>
        @timestamp ${if record.has_key?('@timestamp'); Time.strptime(record["@timestamp"], "%b %d %H:%M:%S").strftime("%Y-%m-%dT%H:%M:%S%z"); end;}
      </record>
    </filter>

    <match os.kern.logs>
      @type copy
      <store>
        @type elasticsearch
        host elasticsearch
        port 9200
        logstash_format true
        logstash_prefix os-kern-logs
        logstash_dateformat %Y%m%d
        include_tag_key true
        tag_key @log_name
        type_name access_log
        flush_interval 2s
      </store>
      <store>
        @type stdout
      </store>
    </match>
  </label>
</worker>

# <worker 2>
#   <source>
#     @type tail
#     path /fluentd/log/auth/syslog
#     read_from_head true
#     <parse>
#       @type syslog
#     </parse>
#     @label @SYSTEM_LABEL
#     tag os.system.logs 
#   </source>

#   <label @SYSTEM_LABEL>
#     <filter os.system.logs>
#       @type grep

#       <regexp>
#         key ident
#         pattern /systemd|kernel/
#       </regexp>
#     </filter>

#     <match os.system.logs>
#       @type copy
#       <store>
#         @type elasticsearch
#         host elasticsearch
#         port 9200
#         logstash_format true
#         logstash_prefix os-system-log
#         logstash_dateformat %Y%m%d
#         include_tag_key true
#         type_name access_log
#         tag_key @log_name
#         flush_interval 2s
#       </store>
#       <store>
#         @type stdout
#       </store>
#     </match>
#   </label>
# </worker>

# <worker 3>
#   <source>
#     @type forward
#     port 24224
#     bind 0.0.0.0
#     @label @NORMAL
#   </source>

#   <label @NORMAL>
#     <filter device.data.log.*>
#       @type record_transformer

#       <record>
#         device_id ${record["device_name"]}-${record["device_id"]}
#       </record>
#       remove_keys device_name
#     </filter>

#     <match device.data.log.*>
#       @type copy
#       <store>
#         @type elasticsearch
#         host elasticsearch
#         port 9200
#         logstash_format true
#         logstash_prefix device-data-log
#         logstash_dateformat %Y%m%d
#         include_tag_key true
#         type_name access_log
#         tag_key @log_name
#         flush_interval 2s
#       </store>
#       <store>
#         @type stdout
#       </store>
#     </match>

#     <match access.log.**>
#       @type copy
#       <store>
#         @type elasticsearch
#         host elasticsearch
#         port 9200
#         logstash_format true
#         logstash_prefix access-log
#         logstash_dateformat %Y%m%d
#         include_tag_key true
#         type_name access_log
#         tag_key @log_name
#         flush_interval 2s
#       </store>
#       <store>
#         @type stdout
#       </store>
#     </match>
#   </label>
# </worker>

# <worker 4>
#   <source>
#     @type tail
#     path /fluentd/log/auth/dmesg
#     read_from_head true
#     <parse>
#      @type regexp
#       expression /^\[\s*(?<timestamp>[^\]]*)\]\s*(?<event>[^:]*):\s(?<message>.*)$/
#     </parse>
#     @label @DMESG
#     tag os.dmesg.logs
#   </source>

#   <label @DMESG>
#     <match os.dmesg.logs>
#       @type copy

#       <store>
#         @type elasticsearch
#         host elasticsearch
#         port 9200
#         logstash_format true
#         logstash_prefix os-dmesg-log
#         logstash_dateformat %Y%m%d
#         include_tag_key true
#         type_name access_log
#         tag_key @log_name
#         flush_interval 2s
#       </store>
#       <store>
#         @type stdout
#       </store>
#     </match>
#   </label>
# </worker>

# <worker 5>
#   <source>
#     @type tail
#     path /fluentd/log/auth/apt/history.log*
#     read_from_head true
#       <parse>
#         @type multiline
#         format_firstline /^Start-Date/
#         format1 /Start-Date:\s+(?<started-time>[^\s]+\s+[^\s]+)\n/
#         format2 /(?<command>.*)\n/
#         format3 /End-Date: (?<ended-time>[^\s]+\s+[^\s]+)\n/
#       </parse>
#     @label @HISTORY
#     tag os.history.logs
#   </source>

#   <label @HISTORY>
#     <match os.history.logs>
#       @type copy
#       <store>
#         @type elasticsearch
#         host elasticsearch
#         port 9200
#         logstash_format true
#         logstash_prefix os-history-log
#         logstash_dateformat %Y%m%d
#         include_tag_key true
#         type_name access_log
#         tag_key @log_name
#         flush_interval 2s
#       </store>
#       <store>
#         @type stdout
#       </store>
#     </match>
#   </label>
# </worker>

# <worker 6>
#   <source>
#     @type tail
#     path /fluentd/log/auth/boot.log
#     read_from_head true
#     <parse>
#      @type regexp
#       expression /^\s*\[?[^\s]*\s*(?<response>[^\s]+)\s*[&\s]*\s*(?<status>[^\s]*)\s*(?<message>.*)$/
#     </parse>
#     @label @BOOT
#     tag os.boot.logs
#   </source>
#   <label @BOOT>
#     <match os.boot.logs>
#       @type copy
#       <store>
#         @type elasticsearch
#         host elasticsearch
#         port 9200
#         logstash_format true
#         logstash_prefix os-boot-log
#         logstash_dateformat %Y%m%d
#         include_tag_key true
#         type_name access_log
#         tag_key @log_name
#         flush_interval 2s
#       </store>
#       <store>
#         @type stdout
#       </store>
#     </match>
#   </label>
# </worker>

# <worker 7>
#   <source>
#     @type tail
#     path /fluentd/log/logs/catalia.out
#     read_from_head true

#     <parse>
#       @type multiline
#       format_firstline /^\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2},\d{3}|\d{2}-[A-Za-z]{3}-\d{4}\s\d{2}:\d{2}:\d{2}\.\d{3}/
#       format1 /^(?<timestamp_1>\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2},\d{3})\s\[(?<thread>[^\]]+)\]\s*(?<log_level>[A-Z]+)\s*(?<class>\S+):(?<line>\d+)\s*-\s*(?<message>.*)$|^(?<timestamp_2>\d{2}-[A-Za-z]{3}-\d{4}\s\d{2}:\d{2}:\d{2}.\d{3})\s*(?<log_level>[^\s]+)\s*\[(?<thread>[^\]]+)\]\s*(?<class>[^\s]+)\.(?<method>[^\s]+)\s\[.*\]\s(?<status>ENTER|EXIT|SQL\s*LOG)\s*:\s*(?<message>.+)$/
#     </parse>

#     @label @CATALINA
#     tag catalina.logs
#   </source>
#   <label @CATALINA>
#     <filter catalina.logs>
#       @type record_transformer
#       enable_ruby true

#       <record>
#         @timestamp ${record.has_key?("timestamp_1") ? Time.strptime(record["timestamp_1"], "%Y-%m-%d %H:%M:%S,%L").strftime("%Y-%m-%dT%H:%M:%S%z") : Time.strptime(record["timestamp_2"], "%d-%b-%Y %H:%M:%S.%L").strftime("%Y-%m-%dT%H:%M:%S%z")}
#       </record>
#       
#       remove_keys timestamp_1,timestamp_2
#     </filter>

#     <match catalina.logs>
#       @type copy
#       <store>
#         @type elasticsearch
#         host elasticsearch
#         port 9200
#         logstash_format true
#         logstash_prefix catalina-logs
#         logstash_dateformat %Y%m%d
#         include_tag_key true
#         type_name access_log
#         tag_key @log_name
#         flush_interval 2s
#       </store>

#       <store>
#         @type stdout
#       </store>
#     </match>
#   </label>
# </worker>

# <worker 8>
#   <source>
#     @type tail
#     path /fluentd/log/logs/apache.log
#     read_from_head true

#     <parse>
#      @type regexp
#       expression /^(?<client.ip>\S+) (?<remote_host>\S+) (?<remote_user>\S+) \[(?<@timestamp>[^\]]+)\] "(?<request.method>\S+)(?: +(?<request.url>(?:[^\"]|\\.)*?)(?: +\S*)?)?" (?<response.status_code>\d+) (?<response.size>\d+)$/
#     </parse>

#     @label @APACHE_LOGS
#     tag apache.log
#   </source>
#   <label @APACHE_LOGS>

#     <filter apache.log>
#       @type record_transformer
#       enable_ruby true
#       <record>
#         @timestamp ${if record.has_key?('@timestamp'); Time.strptime(record["@timestamp"], "%d/%b/%Y:%H:%M:%S %z").strftime("%Y-%m-%dT%H:%M:%S%z"); end;}
#       </record>
#     </filter>
#     <match apache.log>
#       @type copy
#       <store>
#         @type elasticsearch
#         host elasticsearch
#         port 9200
#         logstash_format true
#         logstash_prefix apache-log
#         logstash_dateformat %Y%m%d
#         include_tag_key true
#         type_name access_log
#         tag_key @log_name
#         flush_interval 2s
#       </store>
#       <store>
#         @type stdout
#       </store>
#     </match>
#   </label>
# </worker>

# <worker 9>
#   <source>
#     @type tail
#     path /fluentd/log/logs/nginx.log
#     read_from_head true

#     <parse>
#      @type regexp
#       expression /^(?<client.ip>\S+) (?<remote_host>\S+) (?<remote_user>[^ ]*) \[(?<@timestamp>[^\]]*)\] "(?<request.method>\S+)(?: +(?<request.url>(?:[^\"]|\\.)*?)(?: +\S*)?)?" (?<response.status_code>[^ ]*) (?<response.size>[^ ]*)(?: "(?<referer>(?:[^\"]|\\.)*)" "(?<user.agent>(?:[^\"]|\\.)*)")?$/
#     </parse>

#     @label @NGIN_LOGS
#     tag nginx.log
#   </source>
#   <label @NGIN_LOGS>
#     <filter nginx.log>
#       @type record_transformer
#       enable_ruby true
#       <record>
#         @timestamp ${if record.has_key?('@timestamp'); Time.strptime(record["@timestamp"], "%d/%b/%Y:%H:%M:%S %z").strftime("%Y-%m-%dT%H:%M:%S%z"); end;}
#       </record>
#     </filter>
#     <match nginx.log>
#       @type copy
#       <store>
#         @type elasticsearch
#         host elasticsearch
#         port 9200
#         logstash_format true
#         logstash_prefix nginx-log
#         logstash_dateformat %Y%m%d
#         include_tag_key true
#         type_name access_log
#         tag_key @log_name
#         flush_interval 2s
#       </store>
#       <store>
#         @type stdout
#       </store>
#     </match>
#   </label>
# </worker>
